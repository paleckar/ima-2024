{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c707703",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Upon completion of this lecture, we will**\n",
    "- Understand what digital images are.\n",
    "- Understand how images are acquired and formed.\n",
    "- Understand the basics of image sampling and quantization.\n",
    "- Know which tools to use when working with digital images in Python.\n",
    "- Understand common gotchas of Numpy, Matplotlib and OpenCV.\n",
    "\n",
    "**The lecture is based on**\n",
    "- Chapter 1 of  \n",
    "  [\\[Burger22\\] Wilhelm Burger, Mark J. Burge: \"Digital Image Processing: An Algorithmic Introduction (3rd edition)\"](https://imagingbook.com/books/english-edition-hardcover/)\n",
    "- Chapter 2 of  \n",
    "  [\\[Gonzalez18\\] Rafael C. Gonzalez, Richard E. Woods. \"Digital Image Processing (4th edition)\"](https://www.imageprocessingplace.com/)\n",
    "- Chapter \"Digital images\" of  \n",
    "  [\\[Mazet24\\] Vincent Mazet: \"Basics of Image Processing\"](https://vincmazet.github.io/bip/digital-images/intro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a52c4e",
   "metadata": {},
   "source": [
    "# Basic terms\n",
    "\n",
    "> **Signal processing** is an electrical engineering subfield that focuses on analyzing, modifying and synthesizing signals, such as sound, images, potential fields, seismic signals, altimetry processing, and scientific measurements.\n",
    "\n",
    "&mdash; [Wikipedia](https://en.wikipedia.org/wiki/Signal_processing) \n",
    "\n",
    "> **Digital image processing** is the use of a digital computer to process digital images through an algorithm. It is a subcategory or field of digital signal processing.\n",
    "\n",
    "&mdash; [Wikipedia](https://en.wikipedia.org/wiki/Digital_image_processing)\n",
    "\n",
    "> **Image analysis** or imagery analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques.\n",
    "\n",
    "&mdash; [Wikipedia](https://en.wikipedia.org/wiki/Image_analysis) \n",
    "\n",
    "> **Computer vision** tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.\n",
    "\n",
    "&mdash; *[Wikipedia](https://en.wikipedia.org/wiki/Computer_vision)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e25b5f",
   "metadata": {},
   "source": [
    "# Image formation (acquisition)\n",
    "\n",
    "> The study of image formation encompasses the radiometric and geometric processes by which 2D images of 3D objects are formed. In the case of digital images, the image formation process also includes analog to digital conversion and sampling.\n",
    "\n",
    "&mdash; [Wikipedia](https://en.wikipedia.org/wiki/Image_formation)\n",
    "\n",
    "There are three major components that determine the appearance of an image [\\[Angelopoulou\\]](https://www5.informatik.uni-erlangen.de/fileadmin/lectures/2013s/Lecture.2013s.CV/image_formation.pdf):\n",
    "1. Geometry (of both scene and camera)\n",
    "2. Optical properties (of materials in the scene and of the sensor)\n",
    "3. Illumination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3a7dc",
   "metadata": {},
   "source": [
    "## Pinhole camera model\n",
    "\n",
    "- Camera obscura\n",
    "- Requires very small opening (pinhole) to produce a sharp image, which reduces the amount of light\n",
    "- This must be compensated for by very long exposure times\n",
    "- Not used in practice, but a useful model for understanding\n",
    "- [A real world example](https://web.eecs.umich.edu/~justincj/slides/eecs442/WI2021/442_WI2021_cameras.pdf)\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-pinhole_camera_model.png\" alt=\"\" style=\"width: 6.4in;\"/>\n",
    "  <figcaption>Source: [Burger22], Fig. 1.2</figcaption>\n",
    "</figure>\n",
    "\n",
    "Pinhole camera model results in a **perspective projection**\n",
    "$$\n",
    "x = -f \\cdot \\frac{X}{Z} \\qquad \\textrm{and} \\qquad y = -f \\cdot \\frac{Y}{Z}\n",
    "$$\n",
    "where\n",
    "- $X, Y, Z$ are the scene coordinates (e.g. in meters)\n",
    "- $f$ is the focal length (typically in millimeters)\n",
    "- $x, y$ are image plane coordinates (e.g. in millimeters)\n",
    "\n",
    "In perspective projection\n",
    "- lines in 3D are projected to lines in 2D\n",
    "- circles in 3D are projected to ellipses in 2D\n",
    "- [cross ratio](https://en.wikipedia.org/wiki/Cross-ratio) is preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84528f6",
   "metadata": {},
   "source": [
    "## Thin lens model\n",
    "\n",
    "- A simplified model of real (thick) lenses\n",
    "- A lens is considered to be thin if its thickness is much less than the radii of curvature\n",
    "- The model ignores optical effects due to thickness, e.g. spherical distortion, chromatic abberation, astigmatism, etc.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-cnx_uphysics_35_04_thinlens.png\" alt=\"\" style=\"width: 6.4in;\"/>\n",
    "  <figcaption>\n",
    "    Source: <a href=\"https://openstax.org/books/university-physics-volume-3/pages/2-4-thin-lenses\">OpenStax: University Physics Volume 3</a>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Thin lens camera model **results in the same geometry as the pinhole camera model**, i.e. the perspective projection as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d1dca",
   "metadata": {},
   "source": [
    "## Image sensing\n",
    "\n",
    "- Conversion of the light energy into voltage or current\n",
    "- The output is proportional to the incoming energy that has been collected over some period (exposure time)\n",
    "- The output voltage is amplified by some factor. This can be controlled by ISO setting of the camera.  \n",
    "  The larger the amplification, the less time we need to collect the light.  \n",
    "  However, too short exposure time will lead to low signal-to-noise ratio and noisy images.\n",
    "- The most common basic sensor is a photodiode\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-photo_sensor.png\" alt=\"\" style=\"width: 6.4in;\"/>\n",
    "  <figcaption>Source: [Gonzalez18], Fig. 2.12</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Camera sensors: CCD vs CMOS**\n",
    "- Light sensors are usually organized into a line or rectangular grid-like pattern.\n",
    "- There are two main technogies: **CCD and CMOS**.\n",
    "1. **Charge coupled device (CCD)**\n",
    "   - In CCD sensors, the charge generated by photons striking each sensor is transferred through the array of pixels in a serial manner.\n",
    "   - There is only one shared output node that converts the accumulated charge into voltage.\n",
    "   - Single conversion means that every pixel is amplified equally, which results in uniform gain & brightness, especially in low-light conditions.\n",
    "   - Typically higher power consumption and slower speed as compared to CMOS.\n",
    "   - Usually used in high quality cameras.\n",
    "2. **Complementary metal–oxide–semiconductor (CMOS)**\n",
    "   - Parallel readout architecture, where each pixel in the array has its own amplifier and readout, resulting in higher speed, but also non-uniform gain.\n",
    "   - CMOS sensors typically have lower power consumption compared to CCD sensors, as they only activate the necessary circuitry when needed.\n",
    "   - CMOS sensors can be manufactured using standard CMOS semiconductor fabrication processes, making them more cost-effective to produce in large volumes.\n",
    "   - Mostly used in consumer grade devices (e.g. phones).\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-ccd_vs_cmos.png\" alt=\"\" style=\"width: 6.4in;\"/>\n",
    "  <figcaption>Source: <a href=\"https://www.gatan.com/ccd-vs-cmos\">gatan.com</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd76c1",
   "metadata": {},
   "source": [
    "## Illumination-reflectance model\n",
    "\n",
    "- We'll treat the image projected onto the image plane as a two-dimensional function $g(y,x): \\mathbb{R}^2 \\rightarrow \\mathbb{R}$, i.e.\n",
    "  - input is the position $(y,x)$ in the image plane \n",
    "  - output is the measured light energy at that position (a scalar whose units are not important)\n",
    "- Hence, $g(y,x)$ is non-negative, i.e.\n",
    "  $$\n",
    "  0 \\le g(y,x) \\lt \\infty\n",
    "  $$\n",
    "- Function $g(y, x)$ is characterized by two components:\n",
    "  1. the amount of source **illumination** $l(y,x)$ incident on the scene being viewed, and\n",
    "  2. the amount of illumination **reflected** $r(y,x)$ by the objects in the scene.\n",
    "\n",
    "The illumination-reflectance model assumes a multiplicative effect, i.e.\n",
    " $$\n",
    " g(y,x) = l(y,x) \\cdot r(y,x)\n",
    " $$\n",
    " where\n",
    " - illumination is $0 \\le l(y,x) \\lt \\infty$,\n",
    " - reflectance is $0 \\le r(y,x) \\lt 1$, with $0$ meaning total absorption and $1$ meaning total reflectance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf6cae",
   "metadata": {},
   "source": [
    "## Sampling and quantization\n",
    "\n",
    "In order to convert the observed continous image function $g(y,x)$ into a digital image $f(m,n)$, the folllowing two principal steps must be performed:\n",
    "\n",
    "**1. Spatial sampling**\n",
    "- Refers to the spatial arrangement of the sensor elements.\n",
    "- Each sensor \"samples\" the light energy at its corresponding location in the image plane. For simplicity, we're going to assume pixels arrangement into a two-dimensional rectangular grid and neglect the pixel area and write\n",
    "  $$\n",
    "  h(m,n) = g(m \\cdot \\Delta y, n \\cdot \\Delta x)\n",
    "  $$\n",
    "  where\n",
    "  - $\\Delta x, \\Delta y$ are distances between centers of the single-pixel sensors (sampling intervals),\n",
    "  - $(m,n) \\in \\mathbb{N}^2$ are *discrete* pixel coordinates\n",
    "  - $h(m,n) \\in \\mathbb{R}^+$\n",
    "- For simplicity, we may neglect the pixel area and simply write\n",
    "  \n",
    "- Most often, the sensors are arranged into a regular rectangular grid-like pattern (matrix).  \n",
    "  Other arrangements such as hexagonal grid are possible, but only used in specialized hardware.\n",
    "- The total **image size** (pixel count) is usually expressed as $N \\times M$ (width x height), e.g. $3840 \\times 2160$.  \n",
    "  **Beware the order**, as when the digital image is in matrix form, the convention is height x width, i.e. $M \\times N$, i.e. $2160 \\times 3840$.\n",
    "- We may also need to know **spatial resolution** of the image, which may be expressed as\n",
    "  - line pairs per unit distance (lp/mm)\n",
    "  - dots (pixels) per unit distance, e.g. dots/pixels per inch (DPI/PPI)\n",
    "  - This tells us how fine the sensor grid is and what is the size of each pixel in real-world units.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-mazet_moire.jpg\" alt=\"\" style=\"width: 6.4in; background-color: white;\"/>\n",
    "  <figcaption>Source: <a href=\"https://vincmazet.github.io/bip/digital-images/acquisition.html\">Mazet24</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "**2. Quantization of pixel values**\n",
    "- The output of $f$ is a real number with an infinite amount of possible distinct values.\n",
    "- It is not plausible to store values with infinite precision in computer memory.\n",
    "- Usually, the range of possible output values of $h$ (codomain of $h$) is uniformly discretized into a $p$-bit integer scale, in which\n",
    "  - 0 represents the lowest possible intensity (e.g. black)\n",
    "  - $2^p-1$ represents the highest possible intensity (e.g. white).\n",
    "- This process is called **quantization**.\n",
    "- Formally, if we assume that\n",
    "  $$\n",
    "  h_{\\textrm{min}} \\le h(m,n) \\le h_{\\textrm{max}}\n",
    "  $$\n",
    "  then\n",
    "  $$\n",
    "  f(m,n) = \\textrm{round}\\left( \\frac{2^p - 1}{h_{\\textrm{max}} - h_{\\textrm{min}}} \\cdot (h(m,n) - h_{\\textrm{min}}) \\right)\n",
    "  $$\n",
    "\n",
    "$p$ | Range        | Use\n",
    "----|--------------|----------------------------------------------\n",
    "1   | $\\{0, 1\\}$   | Binary images: document, illustration, fax\n",
    "8   | $[0, 255]$   | Universal and most common: photo, scan, print\n",
    "10  | $[0, 1023]$  | HDR videos for modern displays\n",
    "12  | $[0, 4095]$  | Medicine, satellite imaging, astronomy\n",
    "16  | $[0, 65535]$ | Medicine, satellite imaging, astronomy\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-mazet_quantization.png\" alt=\"\" style=\"width: 6.4in; background-color: white;\"/>\n",
    "  <figcaption>Source: <a href=\"https://vincmazet.github.io/bip/digital-images/acquisition.html\">Mazet24</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-sampling_and_quantization.png\" alt=\"\" style=\"width: 6.4in;\"/>\n",
    "  <figcaption>Source: [Gonzalez18], Fig. 2.16</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b70a90",
   "metadata": {},
   "source": [
    "# Digital images\n",
    "\n",
    "- For example, the result of the image acquistion above could be an integer digital image $f(m,n) \\in \\{0, \\ldots, 2^p-1\\}$.\n",
    "- We may view such image as a two-dimensional discrete function, which gives us a value $f(m,n)$ for each pixel position $(m,n)$.\n",
    "- The position $(m,n)$ is a pair from a set of possible locations $\\{0, \\ldots, M-1\\} \\times \\{0, \\ldots, N-1\\}$.\n",
    "- $m$ denotes row, $n$ denotes column, e.g. $f(0,1)$ means value at 1st row and 2nd column.\n",
    "- The origin $(0, 0)$ is top-left.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-image_coords.png\" alt=\"\" style=\"width: 6.4in;\"/>\n",
    "  <figcaption>Source: [Burger22], Fig. 1.6</figcaption>\n",
    "</figure>\n",
    "\n",
    "**RGB images**\n",
    "\n",
    "- If the image has color, there will be **three values** for each pixel location $(m,n)$, i.e.\n",
    "  $$\n",
    "  f(m,n) \\in \\{0, \\ldots, 2^p-1\\} \\times \\{0, \\ldots, 2^p-1\\} \\times \\{0, \\ldots, 2^p-1\\}\n",
    "  $$\n",
    "- In other words, value of $f$ at each position would be a triple, i.e.\n",
    "  $$\n",
    "  f(m,n) = (R, G, B)\n",
    "  $$\n",
    "  with $R \\in \\{0, \\ldots, 2^p-1\\}$, $B \\in \\{0, \\ldots, 2^p-1\\}$, and $C \\in \\{0, \\ldots, 2^p-1\\}$\n",
    "\n",
    "**Float images**\n",
    "- Sometimes, we also work with images whose values are real numbers, often in the range $[0, 1]$.  \n",
    "  Even though the values are real numbers, they are still limited by the numerical precision of the data type they are stored in the computer memory. This means that the set of possible distinct values is still finite.\n",
    "\n",
    "**Digital image as a discrete function**\n",
    "\n",
    "In general, the acquired *digital image* is a discrete function\n",
    "$$\n",
    "f: \\mathbb{N}^D \\rightarrow \\mathbb{R}^C$$\n",
    "that maps $D$ discrete coordinates\n",
    "$$(m,n,\\ldots) \\in \\{0, \\ldots, M-1\\} \\times \\{0, \\ldots, N-1\\} \\times \\ldots\n",
    "$$\n",
    "to a tuple of $C$ values from a finite set of intensities $(l_1, \\ldots, l_C) \\in \\mathbb{R}^C$\n",
    "$$\n",
    "f(m,n,\\ldots) \\in \\mathbb{R}^C\n",
    "$$\n",
    "\n",
    "In other words image can be seen as an array of $D$ dimensions and size $M \\times N \\times \\ldots$ whose elements contain $C$ numbers\n",
    "[\\[Mazet24\\]](https://vincmazet.github.io/bip/digital-images/definition.html). We may consider the number $C$ as the number of image channels.\n",
    "\n",
    "For example\n",
    "- An 8-bit integer grayscale image with values between 0 (black) and 255 (white) and size $640 \\times 480$ would correspond to\n",
    "  - $D = 2$, since it is a *two*-dimensional image, i.e. each position is a pair $(m,n)$,\n",
    "  - $M = 480$, $N = 640$, since $M$ is the number of rows and $N$ is the number of columns,\n",
    "  - pixel coordinates would be any combination of $m = 0, \\ldots, 479$ and $n = 0, \\ldots, 639$,\n",
    "  - $f(m,n) \\in \\{0, \\ldots, 255\\}$, since it is an 8-bit image,\n",
    "  - $C = 1$, since it only has a single intensity channel (no color), i.e. the value of $f(m,n)$ is a scalar.\n",
    "- RGB color version of the same image would correspond to\n",
    "  - $C = 3$, since there would be three channels R, G and B.\n",
    "- An MRI or CT image would correspond to\n",
    "  - $D = 3$, since it would be a volume,\n",
    "  - $C = 1$, since there is only a single value for each voxel.\n",
    "- A video could correspond to\n",
    "  - $D = 3$, since there would also be time component,\n",
    "  - $C = 3$, if it has color.\n",
    "\n",
    "**Digital image as a matrix**\n",
    "\n",
    "In practice, when working with images in Python libraries such as scikit-image or OpenCV, digital images will be stored and represented as a simple two-dimensional array, a.k.a a matrix\n",
    "$$\n",
    "\\bold{F} = \\begin{bmatrix}\n",
    "    f_{1,1} & \\ldots & f_{1,N} \\\\\n",
    "    \\vdots  & \\ddots & \\vdots  \\\\\n",
    "    f_{M,1} & \\ldots & f_{M,N}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where\n",
    "- $f_{m,n} = f(m,n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97781d65",
   "metadata": {},
   "source": [
    "# Image arithmetics\n",
    "\n",
    "**Image addition**\n",
    "\n",
    "$$\n",
    "f(m,n) = f_1(m,n) + f_2(m,n)\n",
    "$$\n",
    "\n",
    "- Example: blend two images\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-image_addition.svg\" alt=\"\" style=\"width: 6.4in; background-color: white;\"/>\n",
    "  <figcaption>Source: <a href=\"https://vincmazet.github.io/bip/digital-images/operations.html\">Mazet24</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "**Image subtraction**\n",
    "\n",
    "$$\n",
    "f(m,n) = f_1(m,n) - f_2(m,n)\n",
    "$$\n",
    "\n",
    "- Example: detect changes between two images\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-image_subtraction.svg\" alt=\"\" style=\"width: 6.4in; background-color: white;\"/>\n",
    "  <figcaption>Source: <a href=\"https://vincmazet.github.io/bip/digital-images/operations.html\">Mazet24</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "**Image division**\n",
    "\n",
    "$$\n",
    "f(m,n) = \\frac{f_1(m,n)}{f_2(m,n)}\n",
    "$$\n",
    "\n",
    "- Example: correct non-homogeneous illumination\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-image_division.svg\" alt=\"\" style=\"width: 6.4in; background-color: white;\"/>\n",
    "  <figcaption>Source: <a href=\"https://vincmazet.github.io/bip/digital-images/operations.html\">Mazet24</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15faf5",
   "metadata": {},
   "source": [
    "# Popular image file formats\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"../figures/introduction-raster_vs_svg.png\" alt=\"\" style=\"width: 6.4in; background-color: white;\"/>\n",
    "  <figcaption>Source: <a href=\"https://en.wikipedia.org/wiki/SVG\">Wikipedia</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "**Bitmap (BMP)**\n",
    "- Raster graphics file\n",
    "- Supports grayscale, indexed and true color images\n",
    "- Basic Run Length Encoding (RLE) compression\n",
    "\n",
    "**Graphics Interchange Format (GIF)**\n",
    "- Raster graphics file\n",
    "- Supports indexed color at multiple depths, interlaced image loading\n",
    "- Lossless Lempel-Ziv-Welsch (LZW) compression\n",
    "- Can store multiple images in a single file and animate\n",
    "- Popular on the web\n",
    "\n",
    "**Portable Network Graphics (PNG)**\n",
    "- Raster graphics file\n",
    "- Supports up to $3 \\times 16$ bit per pixel, indexed color images, alpha channel\n",
    "- Up to $2^{30} \\times 2^{30}$ pixels\n",
    "- Lossless Phil Katz's ZIP (PKZIP) compression\n",
    "- Should be the default choice for lossless images\n",
    "\n",
    "**Tagged Image File Format (TIFF)**\n",
    "- Raster graphics file\n",
    "- Flexible file format designed to meet the professional needs of diverse fields\n",
    "- Supports large integer depth and floating point pixels\n",
    "- Supports both lossless (LZW, ZIP, CCIT) and lossy compression (JPEG)\n",
    "- Multiple images in single file (pages)\n",
    "- Supports arbitrary metadata\n",
    "\n",
    "**JPEG File Interchange Format (JFIF)**\n",
    "- Joint Photography Experts Group (JPEG)\n",
    "- Raster graphics file\n",
    "- Lossy compression based on Discrete Cosine Transform (DCT), Huffman coding and RLE\n",
    "- Compression about 1 bit per pixel\n",
    "\n",
    "**Scalable Vector Graphics (SVG)**\n",
    "- Vector graphics file\n",
    "- Represented as XML that defines 14 (SVG 1.1) functional areas and feature sets (e.g. paths, shapes, text, etc.)\n",
    "- Supports interactivity, animation, and rich graphical capabilities, making it suitable for both web and print applications\n",
    "- Can have raster image inside\n",
    "- Lossless compression (GZIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca065d",
   "metadata": {},
   "source": [
    "# Tools and libraries for image processing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a85bb",
   "metadata": {},
   "source": [
    "## Sidenote: Markdown\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://www.markdownguide.org/assets/images/markdown-mark-white.svg\" alt=\"\" style=\"width: 1.6in\"/>\n",
    "</figure>\n",
    "\n",
    "https://www.markdownguide.org/\n",
    "\n",
    "\n",
    "Regular text, then **bold text**, even math\n",
    "$$\\frac{a^2}{(\\log \\sqrt p)}$$\n",
    "Code reference may look like `this_is_name_of_variable`\n",
    "Code block:\n",
    "``` python\n",
    "class A:\n",
    "    pass\n",
    "```\n",
    "``` javascript\n",
    "let x = Math.Random()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f44cc4",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://numpy.org/images/logo.svg\" alt=\"\" style=\"width: 1.6in\"/>\n",
    "</figure>\n",
    "\n",
    "https://numpy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c03b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfe8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054bfe0-9f1d-4957-bccd-3198601e9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(12).reshape(3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82fddfc-085d-4191-a5a4-f8ca49218b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x), x.shape, x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2837da-bb2d-4741-99d6-6236849b5239",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d602af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34464a88-900b-4beb-9927-7ff1db8cdcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = x[0, :]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2f5c2-303a-4b3c-80e9-8532749a6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r), r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fa10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 2, 3]\n",
    "l[0] = 4\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (1, 2, 3)\n",
    "t = (4, t[1], t[2])\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d39fa-aa37-433a-af1a-4f312acfe8f9",
   "metadata": {},
   "source": [
    "### View vs copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db78a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bacac1-1012-4a75-bc1d-d019c399a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[0] = -1\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e7b47-6271-4810-9230-0c960aa8f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832b6f9-4f9c-47a2-b308-11c3b208240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_copy = x[0, :].copy()\n",
    "r_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c6f5e-b056-4628-8c3a-88e05b6e0c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_copy[0] = -2\n",
    "r_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a086b-0afc-4261-aaa9-ddc4bf306b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0d86db0-f193-41eb-b100-8010048c1da5",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384514f1-b842-4ab0-99a4-6285d83b0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(2 * 3 * 4).reshape(2, 3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46758cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc138d3-ebf4-4a67-a957-59b075c5ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec4d76-6341-4f80-afa8-c7f3646e2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fc23b-9243-440d-921b-197916a68219",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, :, 1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be52cc-21b9-4dab-9966-1f4f5df74bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, :, [0, 1, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5352ce1-7435-4cfe-b49a-a78cd787dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08e532-6023-42a4-a4bb-af83966d5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, :, [0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4d50a-4d49-4b05-8bd4-d6241ebf83d6",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1d292-2796-4542-bffa-2f277fdaf711",
   "metadata": {},
   "outputs": [],
   "source": [
    "x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6dd47-9073-4524-a4e7-7c37a12f118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(5).reshape(5, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d12933-dbd7-430d-b9b0-9bb812c32215",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.arange(3).reshape(1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392bc6a-5729-4a48-9380-8a6059415d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a70a6-2895-47cd-bcc1-8b139ba32ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(3, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867ad57-723d-4222-907b-bebb20f1bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1716f40-ebe0-4268-b70c-17a982f8ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = x / x.sum(axis=1, keepdims=True)\n",
    "x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c567d-80dd-4400-8dd3-ef5e4a24457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e673d0",
   "metadata": {},
   "source": [
    "## Matplotlib\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://matplotlib.org/_static/logo_dark.svg\" alt=\"\" style=\"width: 1.6in\"/>\n",
    "</figure>\n",
    "\n",
    "https://matplotlib.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc02c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d55675",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0., 1., 100)\n",
    "x = np.sin(2. * np.pi * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ec816",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, x);\n",
    "plt.xlabel('$t$');\n",
    "plt.ylabel('$x(t)$');\n",
    "plt.title('$x = sin(t)$');\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = x.reshape(-1, 1) * x.reshape(1, -1)\n",
    "xx.shape, xx.dtype, xx.min(), xx.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xx);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a012b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = np.meshgrid(t, t)\n",
    "print(tx.shape, tx.dtype, tx.min(), tx.max())\n",
    "print(ty.shape, ty.dtype, ty.min(), ty.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec23e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.plot_surface(tx, ty, xx, cmap='coolwarm', ec='k', lw=0.1, alpha=0.5);\n",
    "ax.set_xlabel('tx');\n",
    "ax.set_ylabel('ty');\n",
    "ax.set_zlabel('xx');\n",
    "# ax.view_init(elev=30, azim=120, roll=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159c1ac",
   "metadata": {},
   "source": [
    "## Seaborn\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://seaborn.pydata.org/_static/logo-wide-lightbg.svg\" alt=\"\" style=\"width: 1.6in\"/>\n",
    "</figure>\n",
    "\n",
    "https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style='darkgrid'):\n",
    "    plt.plot(t, x);\n",
    "    plt.xlabel('$t$');\n",
    "    plt.ylabel('$x(t)$');\n",
    "    plt.title('$x = sin(t)$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7777978",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style='dark'):\n",
    "    plt.imshow(xx);\n",
    "    plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b962db2",
   "metadata": {},
   "source": [
    "## Scikit-image\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://scikit-image.org/_static/img/logo.png\" alt=\"\" style=\"width: 1.6in\"/>\n",
    "</figure>\n",
    "\n",
    "https://scikit-image.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22457b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = skimage.io.imread('../data/fruits.jpg')\n",
    "type(rgb), rgb.shape, rgb.dtype, rgb.min(), rgb.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=plt.figaspect(0.375))\n",
    "im = axes[0].imshow(rgb[:, :, 0])  # R\n",
    "im = axes[1].imshow(rgb[:, :, 1])  # G\n",
    "im = axes[2].imshow(rgb[:, :, 2]);  # B\n",
    "fig.colorbar(im, ax=axes, orientation='horizontal', aspect=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac434e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = skimage.color.rgb2gray(rgb)\n",
    "gray.shape, gray.dtype, gray.min(), gray.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gray)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gray, cmap='gray', vmin=0, vmax=1)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ec19e",
   "metadata": {},
   "source": [
    "## Convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e5cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rgb / 255.);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ae592",
   "metadata": {},
   "source": [
    "`plt.imshow` also examines the `dtype` of the argument and then decides:\n",
    "- `uint8` dtype will be rendered with zeros meaning minimum and 255 meaning maximum value\n",
    "- `float32` or `float` dtype will be rendered with zeros meaning minimum and 1 meaning maximum value\n",
    "\n",
    "Any value of `float` dtype larger than one will be rendered as white too, so if we don't divide by 255, we'll get an almost white output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822236ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rgb.astype(float));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8a4b1",
   "metadata": {},
   "source": [
    "The proper way to convert data type of images in scikit-image is to use `skimage.utils.img_as_xxx` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_f = skimage.util.img_as_float(rgb)  # convert to float dtype (0..1)\n",
    "rgb_b = skimage.util.img_as_ubyte(rgb_f)  # convert back to unsigned byte (0..255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=plt.figaspect(0.5))\n",
    "axes[0].imshow(rgb_f)\n",
    "axes[1].imshow(rgb_b);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05670ec-6fe0-4284-9f46-2c72b0a6fa74",
   "metadata": {},
   "source": [
    "## OpenCV\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://opencv.org/wp-content/uploads/2022/05/logo.png\" alt=\"\" style=\"width: 1.6in\"/>\n",
    "</figure>\n",
    "\n",
    "https://opencv.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr = cv.imread('../data/fruits.jpg')\n",
    "type(bgr), bgr.shape, bgr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a41e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bgr);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b53d2",
   "metadata": {},
   "source": [
    "`plt.imshow` first examines the input argument (`bgr` in our case), and if it's an `ndarray` with shape `(height, width, 3)`, then it will render it as an RGB image, meaning it will expect that `bgr[:, :, 0]` is the red channel, but this is not the case when read using `cv.imread` function. This is the reason that the color channels are in reversed order in the picture above.\n",
    "\n",
    "In order to correct this, we will reverse the channels after we read the image using numpy indexing `[..., ::-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc519569-db41-44ea-9f4f-7ef57e1bf94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_cv = cv.imread('../data/fruits.jpg')[..., ::-1]\n",
    "type(rgb_cv), rgb_cv.shape, rgb_cv.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d59cf-c943-49e3-9a44-1f784859795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rgb_cv);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(rgb - rgb_cv).sum()  # the same image as from skimage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ima23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3206dd6ffaf79ac97f05b707d6259115553a6c5c14c38f6093e1d524a74a10e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
