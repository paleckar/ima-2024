{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object tracking using CamShift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [CamShift](https://docs.opencv.org/4.x/d7/d00/tutorial_meanshift.html) is an algorihm for tracking objects in a sequence of frames **based on its color distribution**.\n",
    "- It assumes that\n",
    "  1. we know object's current position (e.g. at the start of the video),\n",
    "  2. the object of interest has different color distribution than the background\n",
    "- The tracking is based on [histogram backprojection](https://docs.opencv.org/4.x/dc/df6/tutorial_py_histogram_backprojection.html). \n",
    "  - In short, every pixel in the input image is replaced with its probability under the object's histogram.\n",
    "  - The histogram is computed from the hue component of the [HSV color space](https://en.wikipedia.org/wiki/HSL_and_HSV) extracted from the object.\n",
    "- The backprojection step produces a \"heatmap\" (see OpenCV documentation above) image in which\n",
    "  - *high values* mean that object likely is present, since they correspond to colors that are part of the object (and therefore its histogram),\n",
    "  - *low values* mean that object is likely not present, since they correspond to background colors and those are, by assumption, different from the object.\n",
    "- The object's position is updated by [Mean shift](https://en.wikipedia.org/wiki/Mean_shift) algorithm.\n",
    "  - The idea is to find the \"center of gravity\", i.e. the average coordinate of pixels in the current object bounding box, where each coordinate's contribution to the average is weighted by the backprojection at that position.\n",
    "  - The center of gravity $(x_c, y_c)$ computation is based on calculating zeroth and first order moments according to the formula\n",
    "    $$\n",
    "    m_{k,l} = \\sum_{i=i_1}^{i_2}{ \\sum_{j=j_1}^{j_2}{ \\textrm{bp}(i,j)\\cdot i^k\\cdot j^l } }\n",
    "    $$\n",
    "    so that\n",
    "    $$\n",
    "    x_c = \\frac{m_{0,1}}{m_{0,0}} \\qquad y_c = \\frac{m_{1,0}}{m_{0,0}}\n",
    "    $$\n",
    "    where\n",
    "    - $\\textrm{bp}(i,j)$ is the histogram backprojection image\n",
    "    - $j_1, i_1, j_2, i_2$ are the coordinates of the object bounding box in the \"xyxy\" format (top-left and bottom-right corners).\n",
    "- When we know the *new* center of gravity $(x_c, y_c)$, we shift the object's bounding box coordinates $j_1, i_1, j_2, i_2$ based on the difference from the *previous* center of gravity $(x_c^\\textrm{prev}, y_c^\\textrm{prev})$, e.g. for $j_1$\n",
    "  $$\n",
    "  j_1 \\leftarrow j_1 + x_c - x_c^\\textrm{prev} \\\\\n",
    "  \\ldots\n",
    "  $$\n",
    "\n",
    "  <figure class=\"image\">\n",
    "  <img src=\"../figures/camshift-expected_output.png\" alt=\"\" style=\"width: 6.4in;\"/>\n",
    "  <figcaption>Figure 1: Expected output in the first frame of the video. The green rectangle denotes initial bounding box provided by the user.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: implement the `init_camshift`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function takes as argument an BGR image (the entire frame from the video).\n",
    "- It will\n",
    "  1. extract a region of interest (ROI) based on the initial coordinates provided by the user,\n",
    "  2. convert the ROI BGR imgae to HSV,\n",
    "  3. compute and return histogram from the hue component of the ROI.\n",
    "- The initial object's position is represented as a quadruple of 4 numbers `x1, y1, x2, y2`, in which\n",
    "  - `x1, y1` is the top-left corner,\n",
    "  - `x2, y2` is the bottom-right corner of the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_camshift(\n",
    "    bgr: np.ndarray,\n",
    "    xyxy: tuple[float, float, float, float]\n",
    ") -> np.ndarray:\n",
    "    ########################################\n",
    "    # TODO: implement\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # ENDTODO\n",
    "    ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: implement the `camshift_step`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `camshift_step` implements single step of the CamShift algorithm.\n",
    "- It has four inputs\n",
    "  1. `bgr` is the next frame of the video in the BGR format,\n",
    "  2. `hist` is the object's hue histogram obtained by `init_camshift` in the first step,\n",
    "  3. `xyxy` is the current object's position represented as bounding box coordinates in the \"xyxy\" format,\n",
    "  4. `steps` is the number of times the mean shift should repeat between each pair of video frames.\n",
    "- It will return one output\n",
    "  1. `xyxy`, which will represent the updated object's position.\n",
    "- The function should\n",
    "  1. extract ROI from the input BGR image,\n",
    "  2. backproject the object's histogram onto the ROI to produce the \"heatmap\" mentioned above,\n",
    "  3. calculate the heatmap's center of gravity,\n",
    "  4. update the position based on how it differs from the previous center of gravity (the previous center of gravity will simply be the center of the bounding box passed into the function).\n",
    "  5. be aware of image borders and prevent exceptions raising from invalid image coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camshift_step(\n",
    "    bgr: np.ndarray,\n",
    "    hist: np.ndarray,\n",
    "    xyxy: tuple[float, float, float, float],\n",
    "    steps: int = 1\n",
    ") -> tuple[float, float, float, float]:\n",
    "    ########################################\n",
    "    # TODO: implement\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # ENDTODO\n",
    "    ########################################\n",
    "\n",
    "    return xyxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the tracking loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the functions above are implemented correctly, you can run the following code and it should succefully track the object.\n",
    "- You only need to provide an initial position for the object. It should be a bounding box covering the object entirely with as few background pixels as possible.\n",
    "- The code will probably not work in Google colab or other cloud services. You need to run it locally due to OpenCV's way of displaying images.\n",
    "- If you use Google colab or similar, replace OpenCV plotting-related parts with Matplotlib. The code will not be interactive anymore, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture('../data/cup.mp4')\n",
    "\n",
    "x1, x2 = ...\n",
    "y1, y2 = ...\n",
    "\n",
    "ret, bgr = cap.read()\n",
    "h = init_camshift(bgr, ...)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style='darkgrid'):\n",
    "    plt.plot(h);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There should be no need to modify the following code\n",
    "\n",
    "cap = cv.VideoCapture('../data/cup.mp4')\n",
    "\n",
    "x1, x2 = 280, 380\n",
    "y1, y2 = 160, 295\n",
    "box = x1, y1, x2, y2\n",
    "\n",
    "ret, bgr = cap.read()\n",
    "h = init_camshift(bgr, box)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, bgr = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        box = camshift_step(bgr, h, box, steps=3)\n",
    "        \n",
    "        # Draw the tracking results\n",
    "        j1, i1, j2, i2 = [int(0.5 + v) for v in box]\n",
    "        cv.line(bgr, (j1, i1), (j2, i1), (0, 255, 0))\n",
    "        cv.line(bgr, (j2, i1), (j2, i2), (0, 255, 0))\n",
    "        cv.line(bgr, (j2, i2), (j1, i2), (0, 255, 0))\n",
    "        cv.line(bgr, (j1, i2), (j1, i1), (0, 255, 0))\n",
    "        cv.imshow('camshift', bgr)\n",
    "\n",
    "        key = cv.waitKey(0)  # hit any key to continue to the next frame\n",
    "        if key == 27:  # hit escape to break\n",
    "            break\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ima23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
